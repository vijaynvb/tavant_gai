{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dt9_nCtGCJku",
      "metadata": {
        "id": "dt9_nCtGCJku"
      },
      "source": [
        "# **Memory**\n",
        "\n",
        "The memory allows a Large Language Model (LLM) to remember previous interactions with the user. By default, LLMs are stateless — meaning each incoming query is processed independently of other interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224cfbf2",
      "metadata": {
        "id": "224cfbf2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "%pip install --upgrade langchain_classic langchain_community langchain_aws python-dotenv "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab75d1a0",
      "metadata": {
        "id": "ab75d1a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = os.getenv(\"AWS_DEFAULT_REGION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eAiHVbID3D8d",
      "metadata": {
        "id": "eAiHVbID3D8d"
      },
      "source": [
        "# **ConversationBufferMemory**\n",
        "\n",
        "ConversationBufferMemory usage is straightforward. It simply keeps the entire conversation in the buffer memory up to the allowed max limit (e.g. 4096 for gpt-3.5-turbo, 8192 for gpt-4). The main downside, however, is the cost. Each request is sending the aggregation to the API. Since the charge is based on token usages, the cost can quickly add up, especially if you are sending requests to an AI platform. Additionally, there can be added latency given the sheer size of the text that’s being passed back and forth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060ae786",
      "metadata": {
        "id": "060ae786"
      },
      "outputs": [],
      "source": [
        "# ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d72e64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94d72e64",
        "outputId": "177f3aa9-818e-4988-9e5d-0f060e9d1462"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose= True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f30637",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "id": "39f30637",
        "outputId": "cce2a504-0ecc-4c6f-c65e-ec117181a545"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi, my name is Vijay\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7747d73f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "collapsed": true,
        "id": "7747d73f",
        "outputId": "065ef04f-49ae-4b9a-af6a-c3a0956aad36"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4fada3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "collapsed": true,
        "id": "0b4fada3",
        "outputId": "8485219f-42ad-409e-b783-697e71e064e8"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7024a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae7024a8",
        "outputId": "b31beff7-4e45-4bfb-a653-1e6d62c4fef4"
      },
      "outputs": [],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbabbc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbabbc7",
        "outputId": "e556b3b7-f774-443d-a24c-d7f367848151"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5429a5",
      "metadata": {
        "id": "3d5429a5"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b12c48",
      "metadata": {
        "id": "13b12c48"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60cf0bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60cf0bf",
        "outputId": "bc81c5b0-74c1-4af0-897b-f5ea997aa87f"
      },
      "outputs": [],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d43986",
      "metadata": {
        "id": "f1d43986"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134965c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134965c6",
        "outputId": "bdde6329-7a94-4064-c792-c93ba864a1de"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAw8r0j84fpp",
      "metadata": {
        "id": "XAw8r0j84fpp"
      },
      "source": [
        "# **ConversationBufferWindowMemory**\n",
        "\n",
        "Given that most conversations will not need context from several messages before, we also have the option of also using ConversationBufferWindowMemory. With this option we can simply control the buffer with a window of the last k messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uon9I7BH4jIz",
      "metadata": {
        "id": "Uon9I7BH4jIz"
      },
      "outputs": [],
      "source": [
        "# ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hVkGp84F4u7i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVkGp84F4u7i",
        "outputId": "0ee15fb2-9ebf-4451-dbf0-9a2d2561ec76"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose= True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wfQrPrHT4xSD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "wfQrPrHT4xSD",
        "outputId": "a1c6a823-9a4d-4e8b-8e0e-22dfb336054e"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi, my name is Vijay\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lDTGEgXE40zU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "lDTGEgXE40zU",
        "outputId": "68342718-e35f-475c-ac4f-3398f415f586"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8HRZS7Vd43I5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "8HRZS7Vd43I5",
        "outputId": "bb4b8dbd-7346-4a74-b546-9ab343d55ee0"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NtT6rBKg5KGk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NtT6rBKg5KGk",
        "outputId": "a4fa11a7-baff-41af-a0d4-4f0c8f1cb6cf"
      },
      "outputs": [],
      "source": [
        "memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8YmUD1855Tv",
      "metadata": {
        "id": "a8YmUD1855Tv"
      },
      "source": [
        "# **ConversationTokenBufferMemory**\n",
        "\n",
        "ConversationTokenBufferMemory simply maintains the buffer by the token count and does not maintain. There is no summarizing with this approach . In fact, it is similar to ConversationBufferWindowMemory, except instead of flushing based on the last number of messages (k), flushing is based on the number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fqq05C9S57DY",
      "metadata": {
        "id": "fqq05C9S57DY"
      },
      "outputs": [],
      "source": [
        "# ConversationTokenBufferMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W_dVZyWZ6CE0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_dVZyWZ6CE0",
        "outputId": "b6aafbaa-246f-4fc2-92c4-38f88c65ae98"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lPPKMyex6PAw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "5fd8009b67b348cd8ad45fc95e17416d",
            "ca31699f4a1c499495defe87262300f6",
            "549329444ae746cba2a3373650bfcd1e",
            "47b2e084f829490c94e3c331caf3e680",
            "cc81b2135122430db5be0acca19fd1ab",
            "d3340344d05d4cf98fc49495e13d0f98",
            "81ca9789833748ef8654c9b184ba166b",
            "9a371278bc044f4285b960fffedfe740",
            "679746eb78774bf3b51264c33f4623f9",
            "5d6b90466cdb40379b19186bcae87594",
            "c66b748cf79a41f3b95a85733df771e8"
          ]
        },
        "id": "lPPKMyex6PAw",
        "outputId": "dbf150c9-19b9-474a-ea1e-2e8364dc8c79"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "memory.save_context({\"input\": \"ML is what?\"},\n",
        "                    {\"output\": \"MachineLearning!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y-9k_v-G6Pgo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-9k_v-G6Pgo",
        "outputId": "841ef975-2a4d-47ae-fdce-1a31fb107b06"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qldj1uay6Vb6",
      "metadata": {
        "id": "qldj1uay6Vb6"
      },
      "source": [
        "# **ConversationSummaryMemory**\n",
        "\n",
        "ConversationSummaryMemory does not keep the entire history in memory like ConversationBufferMemory. Nor does it maintain a window. Rather, the ConversationSummaryMemory continually summarizes the conversation as it’s happening, and maintains it so we have context from the beginning of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9oRvxZ16RWR",
      "metadata": {
        "id": "a9oRvxZ16RWR"
      },
      "outputs": [],
      "source": [
        "# ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38EeuCte6eaj",
      "metadata": {
        "id": "38EeuCte6eaj"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GUYsy_Ar6bLy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUYsy_Ar6bLy",
        "outputId": "b1ea6b7a-7e4f-42df-bb40-a2e43bd6499b"
      },
      "outputs": [],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9rJ8eOVs6lYF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rJ8eOVs6lYF",
        "outputId": "1c0adebb-68ca-4600-b149-6e6a9edae07d"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sjF2YOyY6pJE",
      "metadata": {
        "id": "sjF2YOyY6pJE"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qWPpJ1s76uyj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "qWPpJ1s76uyj",
        "outputId": "294b8765-0941-4252-a2c5-42600e16aafd"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7veqWQbS609N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7veqWQbS609N",
        "outputId": "7b3dcd10-48fd-4ea0-a970-cb5e87b5c11f"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bzfsH0aCDabD",
      "metadata": {
        "id": "bzfsH0aCDabD"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Explore different memory strategies in LangChain for managing conversational context and improving interaction quality.\n",
        "\n",
        "## **Scenario**\n",
        "\n",
        "You are developing a conversational AI system that interacts with users on various topics, retaining context across multiple messages. Your goal is to implement and compare different memory strategies offered by LangChain to maintain conversation history and enhance user engagement.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* Choose Memory Strategy\n",
        "\n",
        "  * ConversationBufferMemory\n",
        "  * ConversationBufferWindowMemory\n",
        "  * ConversationTokenBufferMemory\n",
        "  * ConversationSummaryMemory\n",
        "\n",
        "* Implement Conversation Chain\n",
        "* Interact with the System\n",
        "* Evaluate"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47b2e084f829490c94e3c331caf3e680": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d6b90466cdb40379b19186bcae87594",
            "placeholder": "​",
            "style": "IPY_MODEL_c66b748cf79a41f3b95a85733df771e8",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "549329444ae746cba2a3373650bfcd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a371278bc044f4285b960fffedfe740",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_679746eb78774bf3b51264c33f4623f9",
            "value": 0
          }
        },
        "5d6b90466cdb40379b19186bcae87594": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fd8009b67b348cd8ad45fc95e17416d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca31699f4a1c499495defe87262300f6",
              "IPY_MODEL_549329444ae746cba2a3373650bfcd1e",
              "IPY_MODEL_47b2e084f829490c94e3c331caf3e680"
            ],
            "layout": "IPY_MODEL_cc81b2135122430db5be0acca19fd1ab"
          }
        },
        "679746eb78774bf3b51264c33f4623f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81ca9789833748ef8654c9b184ba166b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a371278bc044f4285b960fffedfe740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c66b748cf79a41f3b95a85733df771e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca31699f4a1c499495defe87262300f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3340344d05d4cf98fc49495e13d0f98",
            "placeholder": "​",
            "style": "IPY_MODEL_81ca9789833748ef8654c9b184ba166b",
            "value": ""
          }
        },
        "cc81b2135122430db5be0acca19fd1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3340344d05d4cf98fc49495e13d0f98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
